# Machine Learning 
This repository is part of my learning journey in Artificial Intelligence. It contains projects that I completed as part of courses I took during my master’s degree at the University of Bristol, as well as on Coursera through IBM and DeepLearning.ai.

I started with the Exploratory Data Analysis course, which is part of the IBM Machine Learning specialization. This course introduces all the essential concepts needed for data analysis before diving into training machine learning models.

###  Module02 - Retrieving and Cleaning Data
covers approaches for retrieving data from various sources, including SQL, NoSQL, JSON, APIs, and cloud storage. It also focuses on cleaning messy data, identifying duplicates, and removing unnecessary information. The module demonstrates common strategies for handling missing data, such as removing entire rows, imputing missing values, or creating a new category to mask them. It also covers methods for detecting outliers—through plots, statistical techniques, or residual analysis—and strategies for handling them, including removal, imputation, variable transformation, or using models that are resistant to outliers.

###  Module03 — Exploratory Data Analysis (EDA)

This module introduces the core concepts of **Exploratory Data Analysis (EDA)** — a crucial step in any machine learning workflow.  
EDA involves **summarizing the main characteristics of datasets**, often through **statistical methods** and **visualization techniques**, to assess whether the data is ready for modeling or requires further preprocessing.  

Within the lab notebooks in the `Module03` folder, I worked with well-known datasets to practice:

- **Data Exploration**: Reading, examining, and interpreting data using `pandas`.  
- **Data Visualization**: Creating plots with libraries such as `Seaborn`, `Matplotlib`, and `plotly.express` to detect correlations, trends, and outliers.  
- **Data Wrangling**: Filtering, reshaping, and cleaning datasets to prepare them for analysis.  

A key outcome of this module was building familiarity with **feature engineering techniques**, which transform variables to improve model performance and meet statistical assumptions. Some techniques applied include:

-  *Logarithmic and polynomial transformations* for handling skewness and nonlinearity.  
-  *Categorical encoding* (e.g., one-hot encoding).  
-  *Feature scaling* (normalization and standardization).  

By mastering these steps, I gained the ability to **identify data quality issues**, **uncover meaningful patterns**, and **engineer features** that enhance the predictive power of machine learning models.
